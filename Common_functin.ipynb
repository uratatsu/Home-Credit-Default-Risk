{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 共通で使える関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 探索的データ解析(EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 欠損値の調査"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#列の欠損値を調べる関数\n",
    "def missing_values_table(df):\n",
    "    #欠損値合計\n",
    "    mis_val = df.isnull().sum()\n",
    "    #欠損値割合\n",
    "    mis_val_percent = 100 * df.isnull().sum()/len(df)\n",
    "    #結果のテーブルを作成\n",
    "    mis_val_table = pd.concat([mis_val,mis_val_percent],axis=1)\n",
    "    \n",
    "    #列名をリネーム\n",
    "    mis_val_table_ren_columns = mis_val_table.rename(\n",
    "    columns = {0:'Missing Values',1:'% of Total Values'})\n",
    "    \n",
    "    #降順にソート\n",
    "    mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "        mis_val_table_ren_columns.iloc[:,1]!=0].sort_values(\n",
    "    '% of Total Values',ascending=False).round(1)\n",
    "    \n",
    "    #サマリーを表示\n",
    "    print(\"Your selected dataframe has \"+str(df.shape[1])+\" columns.\\n\"\n",
    "         \"There are \"+str(mis_val_table_ren_columns.shape[0])+\n",
    "         \" columns that have missing values.\")\n",
    "    \n",
    "    #欠損情報のデータフレームを返す\n",
    "    return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### カテゴリカル変数のエンコーディング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2値の場合\n",
    "le = LabelEncoder()\n",
    "le_count = 0\n",
    "\n",
    "for col in df_train:\n",
    "    if df_train[col].dtype == 'object':\n",
    "        if len(list(df_train[col].unique())) <= 2:\n",
    "            le.fit(df_train[col])\n",
    "            df_train[col] = le.transform(df_train[col])\n",
    "            df_test[col] = le.transform(df_test[col])\n",
    "            \n",
    "            le_count += 1\n",
    "            \n",
    "print('%d columns were label encoded.' % le_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　3種類以上の場合\n",
    "df_train = pd.get_dummies(df_train)\n",
    "df_test = pd.get_dummies(df_test)\n",
    "\n",
    "print('Training Features shape: ', df_train.shape)\n",
    "print('Testing Features shape: ', df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 変数間の相関をとる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate correlations with the target for a dataframe\n",
    "def target_corrs(df):\n",
    "\n",
    "    # List of correlations\n",
    "    corrs = []\n",
    "\n",
    "    # Iterate through the columns \n",
    "    for col in df.columns:\n",
    "        print(col)\n",
    "        # Skip the target column\n",
    "        if col != 'TARGET':\n",
    "            # Calculate correlation with the target\n",
    "            corr = df['TARGET'].corr(df[col])\n",
    "\n",
    "            # Append the list as a tuple\n",
    "            corrs.append((col, corr))\n",
    "            \n",
    "    # Sort by absolute magnitude of correlations\n",
    "    corrs = sorted(corrs, key = lambda x: abs(x[1]), reverse = True)\n",
    "    \n",
    "    return corrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特徴量エンジニアリング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### メモリの節約(カテゴリカル変数の圧縮)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def return_size(df):\n",
    "    \"\"\"Return size of dataframe in gigabytes\"\"\"\n",
    "    return round(sys.getsizeof(df) / 1e9, 2)\n",
    "\n",
    "def convert_types(df, print_info = False):\n",
    "    \n",
    "    original_memory = df.memory_usage().sum()\n",
    "    \n",
    "    # Iterate through each column\n",
    "    for c in df:\n",
    "        \n",
    "        # Convert ids and booleans to integers\n",
    "        if ('SK_ID' in c):\n",
    "            df[c] = df[c].fillna(0).astype(np.int32)\n",
    "            \n",
    "        # Convert objects to category\n",
    "        elif (df[c].dtype == 'object') and (df[c].nunique() < df.shape[0]):\n",
    "            df[c] = df[c].astype('category')\n",
    "        \n",
    "        # Booleans mapped to integers\n",
    "        elif list(df[c].unique()) == [1, 0]:\n",
    "            df[c] = df[c].astype(bool)\n",
    "        \n",
    "        # Float64 to float32\n",
    "        elif df[c].dtype == float:\n",
    "            df[c] = df[c].astype(np.float32)\n",
    "            \n",
    "        # Int64 to int32\n",
    "        elif df[c].dtype == int:\n",
    "            df[c] = df[c].astype(np.int32)\n",
    "        \n",
    "    new_memory = df.memory_usage().sum()\n",
    "    \n",
    "    if print_info:\n",
    "        print(f'Original Memory Usage: {round(original_memory / 1e9, 2)} gb.')\n",
    "        print(f'New Memory Usage: {round(new_memory / 1e9, 2)} gb.')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データを分割して前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    # 不要な行, 列のフィルタなど、データサイズを削減する処理\n",
    "    return x\n",
    "\n",
    "reader = pd.read_csv(fname, skiprows=[0, 1], chunksize=50)\n",
    "df = pd.concat((preprocess(r) for r in reader), ignore_index=True)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### メモリサイズの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def return_size(df):\n",
    "    \"\"\"Return size of dataframe in gigabytes\"\"\"\n",
    "    return round(sys.getsizeof(df) / 1e9, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多項式の追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多項式にしたいfeatureの取り出し\n",
    "poly_features = df_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\n",
    "poly_features_test = df_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n",
    "\n",
    "# 欠損値埋め\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(strategy = 'median')\n",
    "\n",
    "# yの取り出し\n",
    "poly_target = poly_features['TARGET']\n",
    "\n",
    "# yのドロップ\n",
    "poly_features = poly_features.drop(columns = ['TARGET'])\n",
    "\n",
    "poly_features = imputer.fit_transform(poly_features)\n",
    "poly_features_test = imputer.fit_transform(poly_features_test)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_transformer = PolynomialFeatures(degree = 3) #degreeで次数を決定\n",
    "\n",
    "poly_transformer.fit(poly_features)\n",
    "\n",
    "poly_features = poly_transformer.transform(poly_features)\n",
    "poly_features_test = poly_transformer.transform(poly_features_test)\n",
    "print('Polynomial Features shape:',poly_features.shape)\n",
    "\n",
    "poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 欠損値埋めと標準化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Imputer\n",
    "# Drop the target from the training data\n",
    "if 'TARGET' in df_train:\n",
    "    train = df_train.drop(columns = ['TARGET'])\n",
    "else:\n",
    "    train = df_train.copy()\n",
    "# Feature names\n",
    "features = list(train.columns)\n",
    "# Copy of the testing data\n",
    "test = df_test.copy()\n",
    "# Median imputation of missing values\n",
    "imputer = Imputer(strategy = 'median')\n",
    "# Scale each feature to 0-1\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "# Fit on the training data\n",
    "imputer.fit(train)\n",
    "# Transform both training and testing data\n",
    "train = imputer.transform(train)\n",
    "test = imputer.transform(df_test)\n",
    "# Repeat with the scaler\n",
    "scaler.fit(train)\n",
    "train = scaler.transform(train)\n",
    "test = scaler.transform(test)\n",
    "\n",
    "print('Training data shape: ', train.shape)\n",
    "print('Testing data shape: ', test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature_importanceの可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(df):\n",
    "    \"\"\"\n",
    "    Plot importances returned by a model. This can work with any measure of\n",
    "    feature importance provided that higher importance is better. \n",
    "    特徴量の重要度を返す関数\n",
    "    Args:\n",
    "        df (dataframe): feature importances. Must have the features in a column\n",
    "        called `features` and the importances in a column called `importance\n",
    "    　　features列と、importances列のdataframeを入力\n",
    "        \n",
    "    Returns:\n",
    "        shows a plot of the 15 most importance features\n",
    "        15個の重要なimportanceをプロットする\n",
    "        \n",
    "        df (dataframe): feature importances sorted by importance (highest to lowest) \n",
    "        with a column for normalized importance\n",
    "        重要度でソートしたdataframeを返す\n",
    "        \"\"\"\n",
    "    \n",
    "    # Sort features according to importance\n",
    "    df = df.sort_values('importance', ascending = False).reset_index()\n",
    "    \n",
    "    # Normalize the feature importances to add up to one\n",
    "    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
    "\n",
    "    # Make a horizontal bar chart of feature importances\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    ax = plt.subplot()\n",
    "    \n",
    "    # Need to reverse the index to plot most important on top\n",
    "    ax.barh(list(reversed(list(df.index[:15]))), \n",
    "            df['importance_normalized'].head(15), \n",
    "            align = 'center', edgecolor = 'k')\n",
    "    \n",
    "    # Set the yticks and labels\n",
    "    ax.set_yticks(list(reversed(list(df.index[:15]))))\n",
    "    ax.set_yticklabels(df['feature'].head(15))\n",
    "    \n",
    "    # Plot labeling\n",
    "    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n",
    "    plt.show()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Density Estimate Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kde_target(var_name, df):\n",
    "    corr = df['TARGET'].corr(df[var_name])\n",
    "    avg_repaid = df.ix[df['TARGET'] == 0, var_name].median()\n",
    "    avg_not_repaid = df.ix[df['TARGET'] == 1, var_name].median()\n",
    "    plt.figure(figsize= (12,6))\n",
    "    #kdeplot\n",
    "    sns.kdeplot(df.ix[df['TARGET'] == 0, var_name], label = 'TARGET == 0')\n",
    "    sns.kdeplot(df.ix[df['TARGET'] == 1, var_name], label = 'TARGET == 1')\n",
    "    plt.xlabel(var_name); plt.ylabel('Density'); plt.title('%s Distribution' % var_name)\n",
    "    plt.legend();\n",
    "    #print\n",
    "    print('Median value for loan that was not repaid = %0.4f' % avg_not_repaid)\n",
    "    print('Median value for loan that was repaid =     %0.4f' % avg_repaid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
