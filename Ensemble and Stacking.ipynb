{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# pandas and numpy for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# featuretools for automated feature engineering\n",
    "import featuretools as ft\n",
    "\n",
    "# matplotlit and seaborn for visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.size'] = 22\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings from pandas\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# modeling \n",
    "import lightgbm as lgb\n",
    "\n",
    "# utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Going to use these 5 base models for the stacking\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "from sklearn.svm import SVC\n",
    "# memory management\n",
    "import gc\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"D:\\data\\Jupyter Notebook\\kaggle\\Home-Credit-Default-Risk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前処理済データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "train = pd.read_csv('m_train.csv')\n",
    "test = pd.read_csv('m_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = train.drop('TARGET',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels= train['TARGET']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 別の方法でimputation scaleしてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_names = list(train.columns)\n",
    "train_1_names = list(train_1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Imputer\n",
    "# Impute the domainnomial features\n",
    "imputer = Imputer(strategy = 'median')\n",
    "\n",
    "train_imp = imputer.fit_transform(train_1)\n",
    "test_imp = imputer.transform(test)\n",
    "\n",
    "# Scale the domainnomial features\n",
    "#scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "#train_imp = scaler.fit_transform(train_1)\n",
    "#test_imp = scaler.fit_transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imp = pd.DataFrame(data=train_imp,columns = train_1_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imp = pd.DataFrame(data=test_imp,columns = train_1_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imp['TARGET'] = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\n",
    "y_train = train_labels\n",
    "train_imp = train_imp.drop(['TARGET'], axis=1)\n",
    "x_train = train_imp.values # Creates an array of the train data\n",
    "x_test = test_imp.values # Creats an array of the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    307511.000000\n",
       "mean          0.080729\n",
       "std           0.272419\n",
       "min           0.000000\n",
       "25%           0.000000\n",
       "50%           0.000000\n",
       "75%           0.000000\n",
       "max           1.000000\n",
       "Name: TARGET, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[ 61503  61504  61505 ... 307508 307509 307510]\n",
      "[    0     1     2 ... 61500 61501 61502]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   53.8s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  4.1min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    1.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[     0      1      2 ... 307508 307509 307510]\n",
      "[ 61503  61504  61505 ... 123002 123003 123004]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   54.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  4.2min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    1.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[     0      1      2 ... 307508 307509 307510]\n",
      "[123005 123006 123007 ... 184504 184505 184506]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   53.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  4.1min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    2.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    1.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[     0      1      2 ... 307508 307509 307510]\n",
      "[184507 184508 184509 ... 246006 246007 246008]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   54.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  4.1min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    2.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    1.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[     0      1      2 ... 246006 246007 246008]\n",
      "[246009 246010 246011 ... 307508 307509 307510]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   54.4s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  4.2min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    1.6s finished\n"
     ]
    }
   ],
   "source": [
    "# Kfoldを作成\n",
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "SEED = 0\n",
    "NFOLDS = 5\n",
    "kf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\n",
    "oof_train = np.zeros((ntrain,))\n",
    "oof_test = np.zeros((ntest,))\n",
    "oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "rf = RandomForestClassifier(n_estimators = 200,\n",
    "                            random_state = 50,\n",
    "                            verbose = 1, \n",
    "                            n_jobs = -1)\n",
    "for i, (train_index, test_index) in enumerate(kf):\n",
    "    print(i)\n",
    "    print(train_index)\n",
    "    print(test_index)\n",
    "    x_tr = x_train[train_index]\n",
    "    y_tr = y_train[train_index]\n",
    "    x_te = x_train[test_index]\n",
    "    rf.fit(x_tr, y_tr)\n",
    "    oof_train[test_index] = rf.predict(x_te)\n",
    "    oof_test_skf[i, :] = rf.predict(x_test)\n",
    "\n",
    "oof_test[:] = oof_test_skf.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_first_rf = pd.DataFrame({'rf': oof_train.ravel()})\n",
    "x_test_first_rf = pd.DataFrame({'rf': oof_test.ravel()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[ 61503  61504  61505 ... 307508 307509 307510]\n",
      "[    0     1     2 ... 61500 61501 61502]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   14.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[     0      1      2 ... 307508 307509 307510]\n",
      "[ 61503  61504  61505 ... 123002 123003 123004]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   15.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[     0      1      2 ... 307508 307509 307510]\n",
      "[123005 123006 123007 ... 184504 184505 184506]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   15.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[     0      1      2 ... 307508 307509 307510]\n",
      "[184507 184508 184509 ... 246006 246007 246008]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   15.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[     0      1      2 ... 246006 246007 246008]\n",
      "[246009 246010 246011 ... 307508 307509 307510]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   15.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "# Kfoldを作成\n",
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "SEED = 0\n",
    "NFOLDS = 5\n",
    "kf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\n",
    "oof_train = np.zeros((ntrain,))\n",
    "oof_test = np.zeros((ntest,))\n",
    "oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "et = ExtraTreesClassifier(n_estimators = 300,\n",
    "                            random_state = 50,\n",
    "                            max_depth = 8,\n",
    "                            min_samples_leaf = 2,\n",
    "                            verbose = 1, \n",
    "                            n_jobs = -1)\n",
    "for i, (train_index, test_index) in enumerate(kf):\n",
    "    print(i)\n",
    "    print(train_index)\n",
    "    print(test_index)\n",
    "    x_tr = x_train[train_index]\n",
    "    y_tr = y_train[train_index]\n",
    "    x_te = x_train[test_index]\n",
    "    et.fit(x_tr, y_tr)\n",
    "    oof_train[test_index] = et.predict(x_te)\n",
    "    oof_test_skf[i, :] = et.predict(x_test)\n",
    "\n",
    "oof_test[:] = oof_test_skf.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_first_et = pd.DataFrame({'et': oof_train.ravel()})\n",
    "x_test_first_et = pd.DataFrame({'et': oof_test.ravel()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>et</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>307511.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             et\n",
       "count  307511.0\n",
       "mean        0.0\n",
       "std         0.0\n",
       "min         0.0\n",
       "25%         0.0\n",
       "50%         0.0\n",
       "75%         0.0\n",
       "max         0.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_first_et.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 欠損値埋めされたデータを作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 無限大をnullに置き換え\n",
    "train.replace([np.inf, -np.inf], np.nan)\n",
    "test.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imp = train.fillna(train.median())\n",
    "test_imp = test.fillna(train.median())\n",
    "\n",
    "print('Training data shape: ', train_imp.shape)\n",
    "print('Testing data shape: ', test_imp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#列の欠損値を調べる関数\n",
    "def missing_values_table(df):\n",
    "    #欠損値合計\n",
    "    mis_val = df.isnull().sum()\n",
    "    #欠損値割合\n",
    "    mis_val_percent = 100 * df.isnull().sum()/len(df)\n",
    "    #結果のテーブルを作成\n",
    "    mis_val_table = pd.concat([mis_val,mis_val_percent],axis=1)\n",
    "    \n",
    "    #列名をリネーム\n",
    "    mis_val_table_ren_columns = mis_val_table.rename(\n",
    "    columns = {0:'Missing Values',1:'% of Total Values'})\n",
    "    \n",
    "    #降順にソート\n",
    "    mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "        mis_val_table_ren_columns.iloc[:,1]!=0].sort_values(\n",
    "    '% of Total Values',ascending=False).round(1)\n",
    "    \n",
    "    #サマリーを表示\n",
    "    print(\"Your selected dataframe has \"+str(df.shape[1])+\" columns.\\n\"\n",
    "         \"There are \"+str(mis_val_table_ren_columns.shape[0])+\n",
    "         \" columns that have missing values.\")\n",
    "    \n",
    "    #欠損情報のデータフレームを返す\n",
    "    return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#欠損統計\n",
    "missing_values = missing_values_table(train_imp)\n",
    "missing_values.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kfoldを作成\n",
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "SEED = 0\n",
    "NFOLDS = 5\n",
    "kf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\n",
    "\n",
    "# SKlearnHelperのクラス\n",
    "class SklearnHelper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "        \n",
    "    def train(self,x_train,y_train):\n",
    "        self.clf.train(x_train, y_train)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "    \n",
    "    def predict_proba(self, x):\n",
    "        return self.clf.predict_proba(x)\n",
    "    \n",
    "    def fit(self,x,y):\n",
    "        return self.clf.fit(x,y)\n",
    "    \n",
    "    def feature_importances(self, x,y):\n",
    "        print(self.clf.fit(x,y).feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof(clf, x_train, y_train, x_test):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        print(i)\n",
    "        print(train_index)\n",
    "        print(test_index)\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "        #print(x_tr)\n",
    "        #print(y_tr)\n",
    "        clf.fit(x_tr, y_tr)\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put in our parameters for said classifiers\n",
    "# Random Forest parameters\n",
    "rf_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 500,\n",
    "     'warm_start': True, \n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 6,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features' : 'sqrt',\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "# Extra Trees Parameters\n",
    "et_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators':500,\n",
    "    #'max_features': 0.5,\n",
    "    'max_depth': 8,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# AdaBoost parameters\n",
    "ada_params = {\n",
    "    'n_estimators': 500,\n",
    "    'learning_rate' : 0.75\n",
    "}\n",
    "\n",
    "# Gradient Boosting parameters\n",
    "gb_params = {\n",
    "    'n_estimators': 500,\n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Support Vector Classifier parameters \n",
    "svc_params = {\n",
    "    'kernel' : 'linear',\n",
    "    'C' : 0.025\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 5 objects that represent our 4 models\n",
    "rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\n",
    "et = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\n",
    "ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\n",
    "gb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\n",
    "svc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\n",
    "y_train = train_imp['TARGET'].ravel()\n",
    "train_imp = train_imp.drop(['TARGET'], axis=1)\n",
    "x_train = train_imp.values # Creates an array of the train data\n",
    "x_test = test_imp.values # Creats an array of the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training data shape:',x_train.shape)\n",
    "print('Testing data shape:',x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create our OOF train and test predictions. These base results will be used as new features\n",
    "#et_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\n",
    "#print(\"Extra Trees is complete\")\n",
    "rf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\n",
    "print(\"Random Forest is complete\")\n",
    "#ada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost\n",
    "#print(\"AdaBoost is complete\")\n",
    "#gb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\n",
    "#print(\"Gradient Boost is complete\")\n",
    "#svc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n",
    "#print(\"SVC is complete\")\n",
    "print(\"Training is complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_first = pd.DataFrame( {'et': et_oof_train.ravel(),\n",
    "     'rf': rf_oof_train.ravel()})\n",
    "x_train_first.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_first.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_oof_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train_first = np.concatenate(( et_oof_train, rf_oof_train, svc_oof_train), axis=1)\n",
    "#x_test_first = np.concatenate(( et_oof_test, rf_oof_test, svc_oof_test), axis=1)\n",
    "\n",
    "x_train_first = pd.concat([pd.DataFrame(et_oof_train,columns = ['et']),pd.DataFrame(rf_oof_train,columns = ['rf'])],axis=1)\n",
    "x_test_first = pd.concat([pd.DataFrame(et_oof_test,columns = ['et']),pd.DataFrame(rf_oof_test,columns = ['rf'])],axis=1)\n",
    "\n",
    "#x_train_first = pd.DataFrame({'et': et_oof_train, 'rf': rf_oof_train})\n",
    "#x_test_first = pd.DataFrame({'et': et_oof_test, 'rf': rf_oof_test})\n",
    "\n",
    "print('Training data shape:',x_train_first.shape)\n",
    "print('Testing data shape:',x_test_first.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first levelのpredice(0,1)をトレーニング、テストデータにくっつける\n",
    "x_train_second = pd.concat([train,x_train_first],axis=1)\n",
    "x_test_second = pd.concat([test,x_test_first],axis=1)\n",
    "# TARGET列を再びくっつける\n",
    "x_train_second['TARGET'] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first levelの出力だけを、次の入力とする場合\n",
    "x_train_first['TARGET'] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(features, test_features, encoding = 'ohe', n_folds = 5):\n",
    "    \n",
    "    \"\"\"Train and test a light gradient boosting model using\n",
    "    cross validation. \n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "        features (pd.DataFrame): \n",
    "            dataframe of training features to use \n",
    "            for training a model. Must include the TARGET column.\n",
    "        test_features (pd.DataFrame): \n",
    "            dataframe of testing features to use\n",
    "            for making predictions with the model. \n",
    "        encoding (str, default = 'ohe'): \n",
    "            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n",
    "            n_folds (int, default = 5): number of folds to use for cross validation\n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "        submission (pd.DataFrame): \n",
    "            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n",
    "            predicted by the model.\n",
    "        feature_importances (pd.DataFrame): \n",
    "            dataframe with the feature importances from the model.\n",
    "        valid_metrics (pd.DataFrame): \n",
    "            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the ids\n",
    "    train_ids = features['SK_ID_CURR']\n",
    "    test_ids = test_features['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    labels = features['TARGET']\n",
    "    \n",
    "    # Remove the ids and target\n",
    "    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
    "    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n",
    "    \n",
    "    \n",
    "    # One Hot Encoding\n",
    "    if encoding == 'ohe':\n",
    "        features = pd.get_dummies(features)\n",
    "        test_features = pd.get_dummies(test_features)\n",
    "        \n",
    "        # Align the dataframes by the columns\n",
    "        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n",
    "        \n",
    "        # No categorical indices to record\n",
    "        cat_indices = 'auto'\n",
    "    \n",
    "    # Integer label encoding\n",
    "    elif encoding == 'le':\n",
    "        \n",
    "        # Create a label encoder\n",
    "        label_encoder = LabelEncoder()\n",
    "        \n",
    "        # List for storing categorical indices\n",
    "        cat_indices = []\n",
    "        \n",
    "        # Iterate through each column\n",
    "        for i, col in enumerate(features):\n",
    "            if features[col].dtype == 'object':\n",
    "                # Map the categorical features to integers\n",
    "                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n",
    "                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n",
    "\n",
    "                # Record the categorical indices\n",
    "                cat_indices.append(i)\n",
    "    \n",
    "    # Catch error if label encoding scheme is not valid\n",
    "    else:\n",
    "        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n",
    "        \n",
    "    print('Training Data Shape: ', features.shape)\n",
    "    print('Testing Data Shape: ', test_features.shape)\n",
    "    \n",
    "    # Extract feature names\n",
    "    feature_names = list(features.columns)\n",
    "    \n",
    "    # Convert to np arrays\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "    \n",
    "    # Create the kfold object\n",
    "    k_fold = KFold(n_splits = n_folds, shuffle = False, random_state = 50)\n",
    "    \n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "    \n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "    \n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "    \n",
    "    # Lists for recording validation and training scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "    \n",
    "    # Iterate through each fold\n",
    "    for train_indices, valid_indices in k_fold.split(features):\n",
    "        \n",
    "        # Training data for the fold\n",
    "        train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "        # Validation data for the fold\n",
    "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "        \n",
    "        # Create the model\n",
    "        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', boosting_type='goss',\n",
    "                                   class_weight = 'balanced', learning_rate = 0.05, \n",
    "                                   reg_alpha = 0.1, reg_lambda = 0.1, n_jobs = -1, random_state = 50)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(train_features, train_labels, eval_metric = 'auc',\n",
    "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n",
    "                  early_stopping_rounds = 100, verbose = 200)\n",
    "        \n",
    "        # Record the best iteration\n",
    "        best_iteration = model.best_iteration_\n",
    "        \n",
    "        # Record the feature importances\n",
    "        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
    "        \n",
    "        # Make predictions\n",
    "        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n",
    "        \n",
    "        # Record the out of fold predictions\n",
    "        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n",
    "        \n",
    "        # Record the best score\n",
    "        valid_score = model.best_score_['valid']['auc']\n",
    "        train_score = model.best_score_['train']['auc']\n",
    "        \n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.enable()\n",
    "        del model, train_features, valid_features\n",
    "        gc.collect()\n",
    "        \n",
    "    # Make the submission dataframe\n",
    "    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n",
    "    \n",
    "    # Make the feature importance dataframe\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "    \n",
    "    # Overall validation score\n",
    "    valid_auc = roc_auc_score(labels, out_of_fold)\n",
    "    \n",
    "    # Add the overall scores to the metrics\n",
    "    valid_scores.append(valid_auc)\n",
    "    train_scores.append(np.mean(train_scores))\n",
    "    \n",
    "    # Needed for creating dataframe of validation scores\n",
    "    fold_names = list(range(n_folds))\n",
    "    fold_names.append('overall')\n",
    "    \n",
    "    # Dataframe of validation scores\n",
    "    metrics = pd.DataFrame({'fold': fold_names,\n",
    "                            'train': train_scores,\n",
    "                            'valid': valid_scores}) \n",
    "    \n",
    "    return submission, feature_importances, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission, feature_importances, metrics = model(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(df):\n",
    "    \"\"\"\n",
    "    Plot importances returned by a model. This can work with any measure of\n",
    "    feature importance provided that higher importance is better. \n",
    "    \n",
    "    Args:\n",
    "        df (dataframe): feature importances. Must have the features in a column\n",
    "        called `features` and the importances in a column called `importance\n",
    "        \n",
    "    Returns:\n",
    "        shows a plot of the 15 most importance features\n",
    "        \n",
    "        df (dataframe): feature importances sorted by importance (highest to lowest) \n",
    "        with a column for normalized importance\n",
    "        \"\"\"\n",
    "    \n",
    "    # Sort features according to importance\n",
    "    df = df.sort_values('importance', ascending = False).reset_index()\n",
    "    \n",
    "    # Normalize the feature importances to add up to one\n",
    "    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
    "\n",
    "    # Make a horizontal bar chart of feature importances\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    ax = plt.subplot()\n",
    "    \n",
    "    # Need to reverse the index to plot most important on top\n",
    "    ax.barh(list(reversed(list(df.index[:15]))), \n",
    "            df['importance_normalized'].head(15), \n",
    "            align = 'center', edgecolor = 'k')\n",
    "    \n",
    "    # Set the yticks and labels\n",
    "    ax.set_yticks(list(reversed(list(df.index[:15]))))\n",
    "    ax.set_yticklabels(df['feature'].head(15))\n",
    "    \n",
    "    # Plot labeling\n",
    "    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n",
    "    plt.show()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_sorted = plot_feature_importances(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トレーニング・テストデータの出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('m_train.csv', index = False)\n",
    "test.to_csv('m_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imp.to_csv('m_train_imp.csv', index = False)\n",
    "test_imp.to_csv('m_test_imp.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### アンサンブル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_ensemble = submission.copy().drop(['TARGET'],axis=1)\n",
    "submission_ensemble['TARGET'] = (submission['TARGET'] + x_train_first['et']+ x_train_first['rf'])/3\n",
    "submission_ensemble['SK_ID_CURR']\n",
    "submission_ensemble.to_csv('submission_ensemble.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
